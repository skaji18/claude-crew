# claude-crew configuration

# Framework version
version: "1.0"

# Default model for sub-agents (used when decomposer does not specify)
default_model: sonnet    # haiku / sonnet / opus

# Maximum number of parallel sub-agents
max_parallel: 10

# Maximum retry count when result files are missing
max_retries: 2

# Maximum turns (API round-trips) per sub-agent (maps to Task tool max_turns)
# Note: Task tool has no time-based timeout; turn count is the only control
worker_max_turns: 30

# Maximum cmd execution duration (seconds). Warning logged if exceeded.
# Optional: set to null to disable time tracking.
max_cmd_duration_sec: 1800

# Pre-flight plan validation (Phase 1.5)
# When true, parent launches a reviewer to validate plan.md before Phase 2
plan_validation: false

# Retrospect mechanism (Phase 4)
# Post-mortem & success analysis after cmd completion
retrospect:
  enabled: true                  # false to skip Phase 4 entirely
  filter_threshold: 3.5          # Minimum total score for proposal adoption
  model: sonnet                  # Model for retrospector (sonnet recommended for analysis)
  full_mode:
    max_improvements: 2          # Max improvement proposals (from failure patterns)
    max_skills: 1                # Max skill proposals (from success patterns)
  light_mode:
    max_skills: 2                # Max skill proposals (light mode, success only)
  memory:
    max_candidates_per_cmd: 5    # Max Memory MCP candidates per cmd (aggregator enforces)
    skill_min_score: 12          # Minimum 3-axis score for skill proposals (max=15)

# Learned Preferences (LP) System
# Learns user preferences from interaction patterns to reduce translation cost
lp_system:
  # Core toggles
  enabled: true                  # Enable/disable LP system entirely (boolean)
  collect_signals: true          # Enable/disable signal detection (boolean)
                                 # If false: existing LPs still apply, no new learning

  # Privacy controls
  reset_all: false               # One-shot flag: delete all LP data on next run (boolean)
                                 # Set to true → parent deletes all lp:* entities + internal state
                                 # Then auto-sets: enabled: false, reset_all: false

  # Visibility
  debug_mode: false              # Annotate LP-influenced decisions in output (boolean)
                                 # true → show "[LP applied: ...]" in worker output
                                 # false → silent application (Principle 1)

  # Limits
  lp_cap: 40                     # Maximum active LP entities (integer, range: 30-50)
                                 # When reached, trigger pruning review

  # Context (optional - for cross-project scope handling)
  project_scope: ""              # Optional project context tag (string)
                                 # Examples: "typescript-enterprise", "python-scripting"
                                 # LPs learned here get tagged with this scope

  technology_stack: []           # Optional technology stack list (array of strings)
                                 # Examples: ["TypeScript", "React", "Node.js"]
                                 # Used for auto-inferring LP scope

# Phase-specific custom instructions (optional)
# Appended to phase prompts when non-empty
phase_instructions:
  decompose: ""    # Appended to decomposer prompt
  execute: |
    ## Self-Challenge (Mandatory)

    Before finalizing, apply adversarial review. Adapt depth to task complexity:

    **All tasks (required)**:
    - List 2 failure scenarios. Format: [scenario] | Likelihood: H/M/L | Impact: Critical/High/Moderate

    **Complex tasks (≥3 design alternatives OR novel problem space)**:
    - List 3 core assumptions. For EACH: argue why it's FALSE (not "could be" — disprove it).
    - Propose 1 alternative that contradicts your baseline paradigm (not a variation).
    - Pre-mortem: If this shipped and failed, what was the root cause?
    - Evidence check: Flag claims without empirical data or established best practices.

    **Validation (mandatory)**: Your self-challenge MUST contradict ≥1 baseline claim.
    - BAD: "Assumption X. Why wrong: [argument]. However, design handles it." (sycophantic)
    - GOOD: "Assumption X is FALSE because [evidence]. Alternative: Y. Trade-off: [comparison]."

    If all findings agree with baseline, RE-CHALLENGE with stronger adversarial stance.

    **Format**: Include "## Self-Challenge" section in your result (structured as above).

    **Exemption**: Trivial tasks (<10 min) may reduce to 1 failure scenario only.
  aggregate: ""    # Appended to aggregator prompt
  retrospect: ""   # Appended to retrospector prompt
